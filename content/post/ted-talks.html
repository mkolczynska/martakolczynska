---
title: "Applause and laughter in TED talks with tidytext"
author: "Marta Kołczyńska"
date: 2018-08-22T09:34:00
categories: ["text"]
tags: ["text analysis", "TED talks", "R", "SICSS", "tidytext"]
output: 
  blogdown::html_page:
    toc: true
    toc_depth: 4
---


<div id="TOC">
<ul>
<li><a href="#setup">Setup</a></li>
<li><a href="#tidy-ted-talks">tidy TED talks</a></li>
<li><a href="#applause-lol">Applause, LOL</a></li>
<li><a href="#sentiment">Sentiment</a></li>
</ul>
</div>

<p>This year I spent two weeks of the summer attending the <a href="https://compsocialscience.github.io/summer-institute/2018/helsinki/">Summer Institute for Computational Social Science Parter Site (SICSS)</a> in Tvärminne and Helsinki, Finland, organized by Matti Nelimarkka from Aalto University and the University of Helsinki, assisted by two TAs: Juho Pääkkönen and Pihla Toivanen from the University of Helsinki. I highly recommend it to anyone with background in the social sciences and interested in computer and data sciences, or the other way around!<br />
The summer institute brought together 19 graduate students and postdoctoral researchers representing the social sciences and computer science. The program included a combination of computational and data skills and social science topics. One of the methods we practiced was text analysis using the <a href="https://github.com/juliasilge/tidytext"><code>tidytext</code> package</a>. For a group project we analyzed a collection of tweets. Below is another simple exercise in text analysis with TED talk transcripts inspired by <a href="http://varianceexplained.org/r/tidytext-plots/">David Robinson’s <em>Examining the arc of 100,000 stories: a tidy analysis</em> post</a>.</p>
<div id="setup" class="section level3">
<h3>Setup</h3>
<p>TED talks are short, engaging and often interactive presentations from expert speakers in various disciplies, including the sciences, social sciences, business, and technology. The recordings are available on-line on the <a href="https://www.ted.com">TED website</a> as videos with subtitles in over 100 languages. Here I analyze English language transcripts of the talks using data available from <a href="https://www.kaggle.com/rounakbanik/ted-talks">Kaggle</a>.</p>
<p>I start by downloading and unzipping the <code>tedmain.csv</code> and <code>transcripts.csv</code> files from Kaggle and reading the files into R. After checking the distribution of TED talks over time, I can see that TED talks didn’t really take off until around 2009, so I will select a subset of talks delivered starting with in that year.</p>
<pre class="r"><code>library(tidyverse) # combine and reshape data
library(tidytext) # tokenize
library(stringr) # working with strings
library(lubridate) # working with dates
library(ggExtra) # ggplot extras

tedmain &lt;- read.csv(&#39;ted_main.csv&#39;, stringsAsFactors=FALSE)
tedscripts &lt;- read.csv(&#39;transcripts.csv&#39;, stringsAsFactors=FALSE)
ted &lt;- inner_join(tedmain, tedscripts, by = &quot;url&quot;)
ted$date &lt;- as.POSIXct(ted$film_date,  origin=&quot;1970-01-01&quot;)

ggplot(ted, aes(year(date))) + geom_histogram(col=&quot;darkgoldenrod1&quot;, 
                                              fill=&quot;darkgoldenrod2&quot;, alpha = .3, binwidth = 1)</code></pre>
<p><img src="/post/ted-talks_files/figure-html/packages-1.png" width="672" /></p>
<pre class="r"><code>ted &lt;- filter(ted, date &gt; as.Date(&quot;2009-01-01&quot;))</code></pre>
</div>
<div id="tidy-ted-talks" class="section level3">
<h3>tidy TED talks</h3>
<p>I use the <code>unnest_tokens</code> function from the <a href="https://github.com/juliasilge/tidytext"><code>tidytext</code> package</a> package to split the text (transcript) into separate words. This creates a tidy format data frame with one word per row, which means that instead of a dataset with 2063 records (TED talks) in the post-2009 subset I now have each word in a separate row for a total of 4,033,133 rows. This enables various analyses of the text, including identifying the positions and relative positions of individual words and sentiment analysis.</p>
<pre class="r"><code>ted_words &lt;- ted %&gt;%
  unnest_tokens(word, transcript) %&gt;%
  select(url, word) # select these two variables only

print(c(nrow(ted), nrow(ted_words)))</code></pre>
<pre><code>## [1]    2063 4033133</code></pre>
<p>But first a little exploration. On average TED talks are 1958-word long. The transcript of the shortest talk has 2 words, while the longest talk has over 9000 words.</p>
<pre class="r"><code>ted_words %&gt;% group_by(url) %&gt;% 
  summarise(nwords = n()) %&gt;%
  summarise(mean = mean(nwords),
            median = median(nwords),
            min = min(nwords),
            max = max(nwords)) </code></pre>
<pre><code>## # A tibble: 1 x 4
##    mean median   min   max
##   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
## 1 1958.  1936.     2  9135</code></pre>
<p>A closer look reveals that the talk with the shortest transcript was a dance performance with music and applause, and no words.</p>
<pre class="r"><code>ted_words %&gt;% group_by(url) %&gt;% 
  summarise(nwords = n()) %&gt;%
  filter(nwords == 2) %&gt;%
  left_join(tedscripts) %&gt;%
  select(2,3) </code></pre>
<pre><code>## # A tibble: 1 x 2
##   nwords transcript       
##    &lt;int&gt; &lt;chr&gt;            
## 1      2 (Music)(Applause)</code></pre>
<p>Merging the tokenized talks with the information in <code>tedmain</code> makes it possible to see how the number of words in the transcript is related to the duration of the talk. The correlation is close to perfect, with a few exceptions, such as the already mentioned dance performance. <code>ggMarginal</code> from the <a href="https://cran.r-project.org/web/packages/ggExtra/index.html"><code>ggExtra</code> package</a> creates combined scatter plots with marginal histograms.</p>
<pre class="r"><code>plot &lt;- ted_words %&gt;% group_by(url) %&gt;% 
  summarise(nwords = n()) %&gt;% 
  full_join(ted, by = &quot;url&quot;) %&gt;%
  ggplot(., aes(duration, nwords)) + geom_point(col = &quot;darkcyan&quot;) + 
    labs(title=&quot;TED talks: Number of words vs. duration&quot;, 
         x=&quot;Duration (seconds)&quot;, y=&quot;Number of words&quot;)

ggMarginal(plot, type=&quot;histogram&quot;, fill = &quot;darkgoldenrod2&quot;,
           xparams = list(bins=50), yparams = list(bins=50))</code></pre>
<p><img src="/post/ted-talks_files/figure-html/tokenize4-1.png" width="576" /></p>
<p>To see what were the most frequently used words, it’s necessary to do some more cleaning otherwise the top 5 will consist of “the”, “and”, “to”, “of”, and “a”. After excluding stop words and all strings with digits (like “100th”), the 20 most popular words are more meaningful. Interestingly, there are only nouns and verbs on this list.</p>
<pre class="r"><code>ted_words %&gt;%
  anti_join(stop_words) %&gt;% # eliminate stop words  # remove stop words
  filter(!grepl(&quot;ˆà|â&quot;, word)) %&gt;% # remove unnecessary strings or obvious words
  filter(grepl(&quot;^[[:alpha:]]*$&quot;,word)) %&gt;% # leave strings made of characters only
  group_by(word) %&gt;%
  summarise(n = n()) %&gt;%
  arrange(desc(n)) %&gt;%  # sort by word frequency in a descending order
  head(., 20)</code></pre>
<pre><code>## # A tibble: 20 x 2
##    word         n
##    &lt;chr&gt;    &lt;int&gt;
##  1 people   15442
##  2 time      8181
##  3 laughter  7620
##  4 world     7521
##  5 applause  4689
##  6 life      4574
##  7 lot       3979
##  8 day       3594
##  9 percent   3175
## 10 called    3148
## 11 human     3071
## 12 change    2845
## 13 started   2679
## 14 talk      2618
## 15 idea      2520
## 16 women     2346
## 17 data      2338
## 18 start     2319
## 19 system    2304
## 20 story     2290</code></pre>
</div>
<div id="applause-lol" class="section level3">
<h3>Applause, LOL</h3>
<p>As shown in the “Top 20” list above, TED talk transcripts contain information about non-spoken content, such as <code>laughter</code> and <code>applause</code>. I will analyse the position of laughter and applause during the talks, calculated as the row number of that word divided by the total number of rows in the given talk (a talk is identified with <code>url</code>).<br />
It turns out, not very shockingly, that applause is concentrated at the end of talks, but also happens - less often - during presentations, and there is no clear pattern as to when exactly.<br />
Laughter can happen any time, but is a little more likely at the beginning of the talk, perhaps because some presenters like to start off with a joke.</p>
<pre class="r"><code>ted_words %&gt;%
  group_by(url) %&gt;%
  mutate(word_pos = row_number() / n()) %&gt;%
  filter(str_to_lower(word) == &quot;applause&quot; | str_to_lower(word) == &quot;laughter&quot;) %&gt;%
  ggplot(., aes(word_pos, col = word)) + 
    scale_color_manual(values=c(&quot;darkcyan&quot;, &quot;darkgoldenrod2&quot;)) +
    geom_freqpoly(size = 1.5) + xlim(0, 1) +
    labs(title=&quot;Relative position of applause and laughter&quot;, 
       x=&quot;Position&quot;, y=&quot;Count&quot;) </code></pre>
<p><img src="/post/ted-talks_files/figure-html/applause-laughter-graph-1.png" width="672" /></p>
</div>
<div id="sentiment" class="section level3">
<h3>Sentiment</h3>
<p>The final thing I want to see is how the sentiment of the content changes within talks, using <a href="http://tidytextmining.com/sentiment.html">sentiment analysis</a>. On average sentiment is positive throughout and increases sharply towards the end, in line with the general idea of TED talks as spreading ideas, optimism, and inspiration.</p>
<pre class="r"><code>ted_words %&gt;%
  group_by(url) %&gt;%
  mutate(word_pos = row_number() / n(),
         decile = ceiling(word_pos * 10) / 10) %&gt;%
  group_by(decile, word) %&gt;% 
  summarise(n = n()) %&gt;%
  inner_join(get_sentiments(&quot;afinn&quot;), by = &quot;word&quot;) %&gt;%
  group_by(decile) %&gt;%
  summarize(score = sum(score * n) / sum(n)) %&gt;%
  ggplot(aes(decile, score)) +
    geom_line(size = 1.5, col = &quot;darkcyan&quot;) +
    expand_limits(y = 0) +
    labs(x = &quot;Position&quot;,
       y = &quot;Mean AFINN sentiment score&quot;)</code></pre>
<p><img src="/post/ted-talks_files/figure-html/sentiment-1.png" width="576" /></p>
<p>The TED talk dataset on Kaggle contains more information that would be interesting to analyze, including tags (key words), ratings, the number of comments, but that’s material for another post, perhaps.</p>
</div>
